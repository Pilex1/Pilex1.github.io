<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">

  <link rel="stylesheet" href="/assets/css/styles.css">

  <link rel="icon" type="image/png" href="/assets/img/favicon.png">


  <script src="https://code.jquery.com/jquery-3.3.1.min.js" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
    crossorigin="anonymous"></script>


  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      startup: {
        ready: () => {
          MathJax.startup.defaultReady();
          MathJax.startup.promise.then(() => {
            // console.log('MathJax initial typesetting complete');

            // a hack, but you need this for lines created using \rule to be rendered in the correct colour
            $("rect").removeAttr("fill");
          });
        }
      }

    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.0/math.js"></script>

  
  

</head>

<body>
  <!-- special case to handle the index page -->



    
    

    
    

    
    

    
    
        <h1 class="text-center">Machine Learning</h1>
        <p class="text-center">And human learning too</p>
        
    




<nav class="justify-content-center">
    
        <a href="/index">
            Home
        </a>
    
        <a href="/computer-graphics/index">
            Computer Graphics
        </a>
    
        <a href="/visualizations/index">
            Mathematical Visualizations
        </a>
    
        <a href="/machine-learning/index">
            Machine Learning
        </a>
    
</nav>
  <div class="container">
    <div class="row my-2">
        <div class="col col-lg-6 mx-auto">
            
            
            
            
            
            
            
            
            
            
            
            
            <div class="card">
                <div class="card-body">
                    <h4 class="card-title">Quick links</h4>
                    
                    
                    
                    <a class="d-block" href="/machine-learning/jacobian">Jacobian matrix</a>
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/chain-rule">Chain rule for vector-to-vector functions</a>
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/linear-regression">Linear regression</a>
                    
                    
                    
                    
                    <a class="fw-bold text-light">Neural networks part 1</a>

                    

                    
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/neural-networks-2">Neural networks part 2</a>
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/matrix-calculus">Matrix calculus</a>
                    
                    
                </div>
            </div>
            
            
        </div>
    </div>
    <div class="row">
        <div class="col col-12">
            <h1>Neural networks part 1</h1>
        </div>
    </div>
</div>

  <script id="dsq-count-scr" src="//pilex-github.disqus.com/count.js" async></script>

  <div class="container">
    <div class="row">
      <div class="col col-12">
        <p>There are a lot of articles, blogs, Youtube videos, etc. out there that introduce neural networks, however most of these do not discuss the mathematics behind neural network training in depth, which is what really interested me when I first learn about neural networks. Two valuable resources I found that <em>did</em> discuss this were Andrew Ng’s <a href="https://www.coursera.org/learn/neural-networks-deep-learning">“Neural Networks and Deep Learning” course on Coursera</a> and Michael Nielsen’s <a href="http://neuralnetworksanddeeplearning.com/index.html">online book of the same title</a>, both of which I highly recommend you check out if you are interested in this topic. I wanted to present here my own take on deriving the backpropagation equations for neural network training; my approach involves developing a framework for computing derivatives of matrix valued functions.</p>

<p>I first introduce briefly what a neural network is, so that you will be able to follow along even without any prior knowledge. I then introduce my framework for computing derivatives, and then I derive the backpropagation equations using the framework.</p>

<h2 id="introduction">Introduction</h2>

<p>We start off with a regression problem, similar to what we had for linear regression. As before, suppose we are given $m$ training example pairs $x^{(i)},y^{(i)}$ for $i=1,\cdots,m$ where $x^{(i)}\in\mathbb R^{ n},y^{(i)}\in\mathbb R$ and we want to predict $y$ from $x$. In linear regression we used a linear (technically, affine) function to predict $y$ from $x$ by making the prediction $\hat y=w^T\cdot x$. In order to simplify notation later on, we will write this instead as $\hat y=x^T\cdot w$.</p>

<p>Of course, if the relationship between $y$ and $x$ is non-linear, then this model will not perform very well. Let $z:=x^T\cdot w$. In order to introduce some non-linearity to our model, we introduce a non-linear activation function $g:\mathbb R\to\mathbb R$ that we apply to $z$, which gives us</p>

\[a=g(z)=g(x^T\cdot w)\]

<p>which is called the activation value. Common choices for $g$ include the sigmoid function $g(z)=\frac{1}{1+e^{-z}}$ or the ReLU function $g(z)=\max(0,z)$.</p>

<p>Unlike linear regression, we will use <em>multiple</em> sets of weights. Call the first set of weights $w_1$ and the first activation value $a_1$ so that</p>

\[a_1=g(x^T\cdot w_1).\]

<p>We can apply the same process with a different set of weights $w_2$ and obtain a different activation value</p>

\[a_2=g(x^T\cdot w_2)\]

<p>and so on. Let $n^{[1]}$ denote the number of different weights we used. Then in general we compute</p>

\[a_i=g(x^T\cdot w_i)\]

<p>for $i=1,2,\cdots,n^{[1]}$. To summarise, the setup we have currently looks like this</p>

<div class="d-block text-center fst-italic"> 
    
    <img src="/assets/img/nn-one-layer.png" class="d-block mx-auto mt-2 mb-0 border border-info " width="400" height="" />
    
    
    <div class="caption"><p>One layer neural network.</p>
</div>
</div>

<p>We call this a neural network with one layer. The circles or activation values are the “neurons” or nodes in the network.</p>

<p>We can add more layers by considering all the activation values as a vector $a^{[1]}\in\mathbb R^{n^{[1]}}$, treating it as an input, and repeating the same process. That is, we have a set of new weights, say $w^{[2]}_1,w^{[2]}_2,\cdots,w^{[2]}_{n^{[2]}}$ and we compute the activation values in the second layer as</p>

\[a_i^{[2]}=g^{[2]}(z^{[2]}_i)\]

<p>where</p>

\[z^{[2]}_i:=({a^{[1]}}^T\cdot w_i^{[2]}).\]

<p>The superscript of $[2]$ denotes which layer we are considering so $w^{[2]}$ are the weights of the second layer, $g^{[2]}$ is the activation function in the second layer and $a^{[2]}$ are the activation values in the second layer.</p>

<div class="d-block text-center fst-italic"> 
    
    <img src="/assets/img/nn-two-layer.png" class="d-block mx-auto mt-2 mb-0 border border-info " width="700" height="" />
    
    
    <div class="caption"><p>Two layer neural network</p>
</div>
</div>

<p>We can keep adding more layers to our network by using the activations of the last layer as inputs to the next layer. However, at the end of the day we want to be predicting a single real value $y\in\mathbb R$. Hence for the last layer we have just a single node, and the value of that node is the model’s prediction.</p>

<p>Also, for the last layer we have to pay a bit more attention to the activation function; for example, if the prediction $\hat y$ should lie in $[0,1]$ then we should apply a sigmoid activation function. However if instead the predicted output can be any real number, then an activation function like sigmoid should <em>not</em> be applied as that would restrict the predicted output to always lie in $[0,1]$.</p>

<div class="d-block text-center fst-italic"> 
    
    <img src="/assets/img/nn-final-layer.png" class="d-block mx-auto mt-2 mb-0 border border-info " width="700" height="" />
    
    
    <div class="caption">
</div>
</div>

<h2 id="vectorization">Vectorization</h2>

<p>And that’s all a neural network is! Before we proceed any further, we will vectorize our setup, like we did for linear regression. Recall that we have $m$ training examples pairs $\{(x^{(i)},y^{(i)})\}_{i=1}^m$ for $x_i\in\mathbb R^n,y_i\in\mathbb R$. Later on, we’re going to have to compute $\hat y^{(i)}$ for all $i$ i.e. we need to compute the model’s prediction on each of the training examples $x^{(i)}$. We can do this one by one as per the procedure described above, but it would be better if we could compute all of them in one go.</p>

<p>To accomplish this, arrange the $x^{(i)}$ as rows of a matrix $X$ as we did for linear regression, and similarly arrange the $y^{(i)}$ as rows of a matrix $Y$. Explicitly,</p>

\[X=\begin{bmatrix}
\rule[.5ex]{2.5ex}{0.5pt}{x^{(1)}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\vdots\\
\rule[.5ex]{2.5ex}{0.5pt}{x^{(m)}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\end{bmatrix}.\]

<p>and</p>

\[Y=\begin{bmatrix}
y^{(1)}\\
\vdots\\
y^{(m)}
\end{bmatrix}.\]

<p>Now, we arrange the weights of the first layer as <em>columns</em> of a matrix $W^{[1]}$ i.e.</p>

\[W^{[1]}=\begin{bmatrix}
{\rule[-1ex]{0.5pt}{2.5ex}}&amp;&amp;{\rule[-1ex]{0.5pt}{2.5ex}}\\
w_1^{[1]}&amp;\cdots&amp;w_{n^{[1]}}^{[1]}\\
{\rule[-1ex]{0.5pt}{2.5ex}}&amp;&amp;{\rule[-1ex]{0.5pt}{2.5ex}}
\end{bmatrix}.\]

<p>Matrix multiply $X$ and $W^{[1]}$ to get a matrix</p>

\[Z^{[1]}:=X\cdot W^{[1]}.\]

<p>What’s really neat about this is that because of how matrix multiplication works, we actually have</p>

\[Z^{[1]}=\begin{bmatrix}
\rule[.5ex]{2.5ex}{0.5pt}{z^{(1)[1]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\vdots\\
\rule[.5ex]{2.5ex}{0.5pt}{z^{(m)[1]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\end{bmatrix}\]

<p>i.e. the $i$th row of $Z^{[1]}$ give us the $z$ values in the first layer of training example $i$.</p>

<p>If we then apply the activation function $g^{[1]}$ elementwise to each component of $Z^{[1]}$ we get a matrix</p>

\[A^{[1]}:=g^{[1]}( Z^{[1]})=\begin{bmatrix}
\rule[.5ex]{2.5ex}{0.5pt}{a^{(1)[1]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\vdots\\
\rule[.5ex]{2.5ex}{0.5pt}{a^{(m)[1]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\end{bmatrix}\]

<p>and similar to before, the $i$th row of $A^{[1]}$ gives us the activation values in the first layer of training example $i$.</p>

<p>We can do this process for all layers so that in general, at layer $\ell$ we have</p>

\[\begin{align*}
Z^{[\ell]}&amp;:=A^{[\ell-1]}\cdot W^{[\ell]}=\begin{bmatrix}
\rule[.5ex]{2.5ex}{0.5pt}{z^{(1)[\ell]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\vdots\\
\rule[.5ex]{2.5ex}{0.5pt}{z^{(m)[\ell]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\end{bmatrix}\\
A^{[\ell]}&amp;:=g^{[\ell]}( Z^{[\ell]})=\begin{bmatrix}
\rule[.5ex]{2.5ex}{0.5pt}{a^{(1)[\ell]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\vdots\\
\rule[.5ex]{2.5ex}{0.5pt}{a^{(m)[\ell]}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\end{bmatrix}
\end{align*}.\]

<p>For the last layer $L$ the activation values $A^{[L]}$ are the model’s predicted values, which are conveniently laid out in the same shape as $Y$ (the $i$th row contains the predicted/actual value in $A^{[L]}$/$Y$). We will see shortly that, like linear regression, we want to minimize the mean squared error between the predicted and actual values over all the training examples. This error is given by</p>

\[\frac 1m\sum_{i=1}^m(\hat y ^{(i)}-y^{(i)})^2\]

<p>which, using our vectorized form, we can write as</p>

\[\frac 1m \Vert A^{[L]}-Y\Vert_F^2\]

<p>where $\Vert\cdot\Vert_F$ denotes Frobenius norm (in this case this is the exact same as the $L_2$ norm but it will be helpful to think of vectors as matrices where possible).</p>

<h2 id="gradient-descent">Gradient descent</h2>

<p>With that out of the way, we now move on to the key issue of neural networks: figuring out what we should set the various weights $W^{[1]},\cdots, W^{[L]}$ to. As alluded to earlier, in our setup for linear regression we picked the weights $w$ so as to minimize the mean squared error between the predicted and actual value over all the training examples</p>

\[f(w)=\frac 1m\sum_{i=1}^m(\hat y ^{(i)}-y^{(i)})^2\]

<p>where $\hat y^{(i)}=x^T\cdot w$ denotes the predicted value by the linear regression model and is dependent on $w$, and $y^{(i)}$ denotes the true value. In other words our solution was</p>

\[w^*=\arg\min_wf(w)\]

<p>which we were able to compute explicitly by finding the critical points of $f$.</p>

<p>In our setup with neural networks we again want to minimze the mean squared error between the predicted and actual values acros all the training examples</p>

\[f(W)=\frac 1m\sum_{i=1}^m(\hat y ^{(i)}-y^{(i)})^2\]

<p>where $\hat y^{(i)}$ denotes the predicted value using our neural network and is dependent on the weights $W:=W^{[1]},\cdots, W^{[L]}$. This time around, however, we are not able to explicitly solve for</p>

\[W^*=\arg\min_wf(W)\]

<p>by finding the critical points of $f$ like we did before. Instead, the trick is to approximate $W^*$ by using gradient descent. To do this we need to compute the partial derivatives of $f$ with respect to all the weights. This means that for the matrix $W^{[\ell]}$ at each layer $\ell$ we need to compute the partial derivative of $f$ with respect to each of the components in $W^{[\ell]}$ i.e. we need to compute</p>

\[\frac{\partial f}{\partial W^{[\ell]}_{i,j}}\]

<p>for all $i,j,\ell$. This can be done in principal, but to make our lives easier, we will try to take the partial derivative with respect to all the components in $W^{[\ell]}$ in one go. Specifically, let $\frac{\partial f}{\partial W^{[\ell]}}$ denote the matrix containing the partial derivatives with respect to each component of $W^{[\ell]}$ (so that the $i,j$th entry of $\frac{\partial f}{\partial W^{[\ell]}}$ is $\frac{\partial f}{\partial W^{[\ell]}_{i,j}}$), and analogously for $\frac{\partial f}{\partial Z^{[\ell]}},\frac{\partial f}{\partial A^{[\ell]}}$, etc. We would like to compute these quantities using only operations between matrices.</p>

<h2 id="matrix-calculus">Matrix calculus</h2>

<p>Our function $f$ is essentially comprised of a series of matrix-to-matrix operations (e.g. matrix multiplication, or applying an elementwise function to a matrix), followed by a matrix-to-scalar function at the end (i.e. computing the Frobenius norm of a matrix).</p>

<p>We will abstract this as follows: let $G$ be an arbitrary matrix-to-matrix function which takes input $X$, and $f$ be an arbitrary matrix-to-scalar function that takes as input $G$ i.e. we have $f(G(X))$ which is matrix-to-scalar.</p>

<p>Our goal is to find a set of rules for computing
\(\frac{df}{dX}\)
in terms of the matrix $\frac{df}{dG}$ and the function $G$.</p>

<h3 id="matrix-to-matrix-derivatives">Matrix-to-matrix derivatives</h3>

<p>To do this, we first let $f$ be arbitrary and fix $G$. This is best explained by example. Below, $G:\mathbb R^{m\times n}\to\mathbb R^{m\times p}$ is the matrix-to-matrix function that takes its input $X$ and right-multiplies it by a constant matrix $W$ so that $G:X\mapsto X\cdot W$. For this particular $G$, we can calculate $\frac{df}{dX}$ as follows.</p>

<h4 id="right-matrix-multiplication">Right Matrix multiplication</h4>

\[\begin{align*}
\frac{df}{dX_{i,j}}&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot\frac{dG_{k,\ell}}{dX_{i,j}}\tag{multivariable chain rule}\\
&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot\frac{d}{dX_{i,j}}(X_{k,1}W_{1,\ell}+X_{k,2}W_{2,\ell}+\cdots+X_{k,n}W_{n,\ell})\\
&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot 1_{k=i}\frac{d}{dX_{i,j}}(X_{k,j}W_{j,\ell})\\
&amp;=\sum_\ell \frac{df}{dG_{i,\ell}}\cdot \frac{d}{dX_{i,j}}(X_{i,j}W_{j,\ell})\\
&amp;=\sum_\ell \frac{df}{dG_{i,\ell}}\cdot W_{j,\ell}\\
&amp;=\sum_\ell \frac{df}{dG_{i,\ell}}\cdot W_{\ell,j}^T\\
\implies \frac{df}{dX}&amp;=\frac{df}{dG}\cdot W^T.
\end{align*}\]

<p>This is essentially a sort of “chain rule”. We’ve written the derivative $\frac{df}{dX}$ in terms of $\frac{df}{dG}$ and $W$ (which comes from the function $G$). In a proper chain rule, we’d expect to see $\frac{dG}{dX}$ appearing somewhere; the right multiplication of $\frac{df}{dG}$ by $W^T$ takes on this role. It is not immediately clear how to define $\frac{dG}{dX}$ since $G$ is matrix-to-matrix and so $\frac{dG}{dX}$ would have to be 4 dimensional; hence I find it easier to write it in the form I have as above.</p>

<h4 id="left-multiplication-by-matrix">Left multiplication by matrix</h4>

<p>Using a similar line of reasoning, one can show (exercise to the reader) that if</p>

\[\begin{align*}
G:X\mapsto W\cdot X\\
G:\mathbb R^{m\times n}\to\mathbb R^{p\times n}
\end{align*}\]

<p>where $W\in\mathbb R^{p\times m}$ is a constant, then</p>

\[\frac{df}{dX}=W^T\cdot\frac{df}{dG}.\]

<p>Notice here that our function $G$ is different; consequently the role of $\frac{dG}{dX}$ is now taken up by <strong>left</strong>-multiplying $\frac{df}{dG}$ by $W^T$.</p>

<h4 id="elementwise-function-application">Elementwise function application</h4>

<p>Here’s a different example that involves applying a scalar-to-scalar function $g:\mathbb R\to\mathbb R$ elementwise to each entry of a matrix.</p>

<p>Let</p>

\[\begin{align*}
G:X\mapsto g(X)\\
g:\mathbb R\to\mathbb R,G:\mathbb R^{m\times n}\to\mathbb R^{m\times n}.
\end{align*}\]

<p>Then</p>

\[\begin{align*}
\frac{df}{dX_{i,j}}&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot\frac{dG_{k,\ell}}{dX_{i,j}}\tag{multivariable chain rule}\\
&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot\frac{d}{dX_{i,j}}g(X_{k,\ell})\\
&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot 1_{k=i,\ell=j}\frac{d}{dX_{i,j}}g(X_{k,\ell})\\
&amp;=\frac{df}{dG_{i,j}}\cdot\frac{d}{dX_{i,j}}g(X_{i,j})\\
&amp;=\frac{df}{dG_{i,j}}\cdot g'(X_{i,j})\\
\implies\frac{df}{dX}&amp;=\frac{df}{dG}\odot (g' (X)).
\end{align*}\]

<p>where $\odot$ denotes the elementwise (Hadamard) product between two matrices. I want to point out again that now the role of $\frac{dG}{dX}$ is to elementwise multiply $\frac{df}{dG}$ by $g’(X)$.</p>

<h4 id="matrix-addition">Matrix addition</h4>

<p>Here’s one final (and simple) example that involves adding a constant matrix.</p>

<p>Let</p>

\[\begin{align*}
G:X\mapsto X+C\\
G:\mathbb R^{m\times n}\to\mathbb R^{m\times n}
\end{align*}\]

<p>where $C\in\mathbb R^{m\times n}$ is a constant. Then</p>

\[\begin{align*}
\frac{df}{dX_{i,j}}&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot\frac{dG_{k,\ell}}{dX_{i,j}}\tag{multivariate chain rule}\\
&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot\frac{d}{dX_{i,j}}(X_{k,\ell}+C_{k,\ell})\\
&amp;=\sum_{k,\ell}\frac{df}{dG_{k,\ell}}\cdot 1_{k=i,\ell=j}\frac{d}{dX_{i,j}}(X_{k,\ell}+C_{k,\ell})\\
&amp;=\frac{df}{dG_{i,j}}\cdot\frac{d}{dX_{i,j}}(X_{i,j}+C_{i,j})\\
&amp;=\frac{df}{dG_{i,j}}\\
\implies \frac{df}{dX}&amp;=\frac{df}{dG}.
\end{align*}\]

<p>This is just a fancy way of saying that adding constants does not change the derivative.</p>

<h3 id="matrix-to-scalar-derivatives">Matrix-to-scalar derivatives</h3>

<p>So far we have written $\frac{df}{dX}$ in terms of $\frac{df}{dG}$. I now explain how to compute $\frac{df}{dG}$. We now let $G$ be arbitrary and fix $f$. In the below example $f$ computes the squared Frobenius norm.</p>

<h4 id="squared-frobenius-norm">Squared Frobenius norm</h4>

<p>Let</p>

\[\begin{align*}
f:G\mapsto \Vert G\Vert_F^2\\
f:\mathbb R^{m\times n}\to\mathbb R
\end{align*}\]

<p>Then</p>

\[\begin{align*}
\frac{df}{dG_{i,j}}&amp;=\frac{d}{dG_{i,j}}\sum_{k,\ell}G_{k,\ell}^2\\
&amp;=\frac{d}{dG_{i,j}}G_{i,j}^2\\
&amp;=2G_{i,j}\\
\implies \frac{df}{dG}&amp;=2G.
\end{align*}\]

<h2 id="derivative-calculations">Derivative calculations</h2>

<p>We now have all the pieces available to compute the derivatives in our neural network.</p>

<h3 id="derivative-with-respect-to-al">Derivative with respect to $A^{[L]}$</h3>

<p>Recall that if we have the model’s predictions $A^{[L]}$ we define the loss to be</p>

<p>\(f(A^{[L]})=\frac 1m\Vert A^{[L]}-Y\Vert_F^2\)
Let’s calculate</p>

\[\frac{df}{dA^{[L]}}.\]

<p>We have</p>

\[\begin{align*}
\frac{df}{dA^{[L]}}&amp;=\frac{d(\frac 1m\Vert A^{[L]}-Y\Vert_F^2)}{dA^{[L]}}\\
&amp;=\frac 1m\frac{d(\Vert A^{[L]}-Y\Vert_F^2)}{dA^{[L]}}\\
&amp;=\frac 1m\frac{d(\Vert A^{[L]}-Y\Vert_F^2)}{d(A^{[L]}-Y)}\\
&amp;=\frac 1m\cdot 2(A^{[L]}-Y).\\
\end{align*}\]

<p>To get to the second last line we have used our matrix calculus framework with the matrix-to-matrix function $G:A^{[L]}\mapsto A^{[L]}-Y$. To get to the last line, we have again used our matrix calculus framework, this time with the matrix-to-scalar function $f:A^{[L]}-Y\mapsto \Vert A^{[L]}-Y\Vert_F^2$.</p>

<h3 id="derivative-with-respect-to-zl">Derivative with respect to $Z^{[L]}$</h3>

<p>Recall that $A^{[L]}=g^{[L]}(Z^{[L]})$ where $g^{[L]}$ is the activation function of the last layer. We can calculate</p>

\[\frac{df}{dZ^{[L]}}\]

<p>using our matrix calculus framework since</p>

\[\begin{align*}
\frac{df}{dZ^{[L]}}&amp;=\frac{df}{dA^{[L]}}\odot {g^{[L]}}'(Z^{[L]})\\
&amp;=\frac 2m (A^{[L]}-Y)\odot ({g^{[L]}}'(Z^{[L]})).
\end{align*}\]

<p>The first equality is due to the matrix calculus framework with matrix-to-matrix function $G:Z^{[L]}\mapsto g^{[L]}(Z^{[L]})$ and the second equality is due to substituting our computation for $\frac{df}{dA^{[L]}}$ that we just did above.</p>

<h3 id="derivative-with-respect-to-wl">Derivative with respect to $W^{[L]}$</h3>

<p>We can now calculate</p>

\[\frac{df}{dW^{[L]}}\]

<p>which is one of the terms that we need to calculate for gradient descent. Recalling that $Z^{[L]}=A^{[L-1]}\cdot W^{[L]}$, we have</p>

\[\begin{align*}
\frac{df}{dW^{[L]}}&amp;={A^{[L-1]}}^T\cdot\frac{df}{dZ^{[L]}}\\
&amp;=\frac 2m {A^{[L-1]}}^T\cdot (A^{[L]}-Y)\odot ({g^{[L]}}'(Z^{[L]})).
\end{align*}\]

<p>The first equality is due to the matrix calculus framework with matrix-to-matrix function $G:W^{[L]}\mapsto A^{[L-1]}\cdot W^{[L]}$ and the second equality is due to substituting the result above.</p>

<h3 id="derivatives-in-an-arbitrary-layer">Derivatives in an arbitrary layer</h3>

<p>It’s now pretty easy to calculate the derivatives in any arbitrary layer. First, let’s calculate $\frac{df}{dA^{[\ell]}}$ for a non-final layer $\ell=1,2,\cdots,L-1$. Notice that $A^{[\ell]}$ is used as the “input” for the next layer $\ell+1$ i.e.</p>

\[Z^{[\ell+1]}=A^{[\ell]}\cdot W^{[\ell+1]}.\]

<p>Our matrix calculus framework then tells us</p>

\[\frac{df}{dA^{[\ell]}}=\frac{df}{dZ^{[\ell+1]}}\cdot {W^{[\ell+1]}}^T\]

<p>We can calculate the derivatives with respect to $Z^{[\ell]}$ and $W^{[\ell]}$ in the exact same way that we calculated the derivatives for $Z^{[L]}$ and $W^{[L]}$ respectively (check yourself), so we get</p>

\[\frac{df}{dZ^{[\ell]}}=\frac{df}{dA^{[\ell]}}\odot (g'( Z^{[\ell]}))\\
\frac{df}{dW^{[\ell]}}={A^{[\ell-1]}}^T\cdot\frac{df}{dZ^{[\ell]}}\]

<h2 id="derivative-summary">Derivative summary</h2>

<p>To summarise, our matrix calculus framework allowed us to compute the derivatives of the loss function with respect to any matrix quantity in our neural network. Explicitly, we have</p>

\[\begin{align*}
\frac{df}{dA^{[L]}}=\frac 1m\cdot 2(A^{[L]}-Y)\\
\frac{df}{dA^{[\ell]}}=\frac{df}{dZ^{[\ell+1]}}\cdot {W^{[\ell+1]}}^T,&amp;\qquad \ell =1,2,\cdots,L-1\\
\frac{df}{dZ^{[\ell]}}=\frac{df}{dA^{[\ell]}}\odot (g'( Z^{[\ell]})),&amp;\qquad \ell=1,2,\cdots,L\\
\frac{df}{dW^{[\ell]}}={A^{[\ell-1]}}^T\cdot\frac{df}{dZ^{[\ell]}},&amp;\qquad\ell=1,2,\cdots,L
\end{align*}\]

<h2 id="final-notes">Final notes</h2>

<p>Note that although we only needed to calculate $\frac{df}{dW^{[\ell]}}$, we also calculated $\frac{df}{dA^{[\ell]}}$ and $\frac{df}{dZ^{[\ell]}}$ as they are required in intermediate calculations.</p>

<p>Now that we’ve calculated $\frac{df}{dW^{[\ell]}}$, we can apply gradient descent to approximate $W^*$, the optimal set of weights that result in the lowest MSE across all the training examples. To do this, typically all the weights are initialized randomly, and the weights are updated as</p>

\[W^{[\ell]}\leftarrow W^{[\ell]}-\alpha \cdot \frac{df}{dW^{[\ell]}}\]

<p>where $\alpha&gt;0$ is the learning rate. In practice, variations of the traditional gradient descent update rule are used instead (e.g. Adam), though we will not discuss about them in depth here.</p>

      </div>
    </div>
    
    
    <div class="row">
      <div class="col col-12">
        
        <div id="disqus_thread" class="my-5"></div>
        <script>
          /**
          *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
          *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
          var disqus_config = function () {
            this.page.url = "/machine-learning/neural-networks-1.html";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = "Neural Networks 1"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
          };
          (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://pilex-github.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        

      </div>
    </div>
  </div>

  <script>
    $("table").attr("class", "table");
  </script>

  <footer>
    <hr>
    <p class="centered"
      style="margin: 15px; margin-left: auto; margin-right: auto; font-size: 14px; text-align: center">
      Copyright Alex Tan &copy; 2022
    </p>
  </footer>
</body>

</html>