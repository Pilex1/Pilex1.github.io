<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">

     <link rel="stylesheet" href="/assets/css/styles.css">

     <link rel="icon" type="image/png" href="/assets/img/favicon.png">


  <script src="https://code.jquery.com/jquery-3.3.1.min.js" crossorigin="anonymous"></script>
 
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>


  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.0/math.js"></script>

    
    

</head>

<body>
  <!-- special case to handle the index page -->



    
    

    
    

    
    

    
    
        <h1 class="text-center">Machine Learning</h1>
        <p class="text-center">And human learning too</p>
        
    




<nav class="justify-content-center">
    
        <a href="/index">
            Home
        </a>
    
        <a href="/computer-graphics/index">
            Computer Graphics
        </a>
    
        <a href="/visualizations/index">
            Mathematical Visualizations
        </a>
    
        <a href="/machine-learning/index">
            Machine Learning
        </a>
    
</nav>
  <div class="container">
    <div class="row my-2">
      <div class="col col-lg-6 mx-auto">
        
        
        
    
        
        
    
        
        
    
        
        
            <div class="card">
                <div class="card-body">
                    <h4 class="card-title">Quick links</h4>
                    
                        
                                <a class="d-block" href="/machine-learning/jacobian">Jacobian matrix</a>
                        
                    
                        
                                <a class="d-block" href="/machine-learning/chain-rule">Chain rule for vector valued functions</a>
                        
                    
                        
                            <a class="fw-bold text-light">Linear regression</a>
                            
                            
                    
                        
                                <a class="d-block" href="/machine-learning/neural-networks">Neural networks</a>
                        
                    
                </div>
            </div>
        
    
      </div>
    </div>
    <div class="row">
        <div class="col col-12">
            <h1>Linear regression</h1>
        </div>
    </div>
</div>




  <script id="dsq-count-scr" src="//pilex-github.disqus.com/count.js" async></script>

  <div class="container">
    <div class="row">
      <div class="col col-12">
        <p>In linear regression we are given $m$ training example pairs $x^{(i)},y^{(i)}$ for $i=1,\cdots,m$ where $x^{(i)}\in\mathbb R^n,y^{(i)}\in\mathbb R$ and we want to predict $y$ from $x$ using a linear function. More precisely, we want to find weights $w\in\mathbb R^n$ so as to minimize the mean square error over all training pairs between the predicted value $\hat y^{(i)}=w^T\cdot x^{(i)}$ and the actual value $y^{(i)}$, that is, we want to find</p>

\[\hat w:=\arg\min_w\frac 1m\sum_{i=1}^m(w^T\cdot x^{(i)}-y^{(i)})^2.\]

<p>In practice, we allow ourselves to consider affine functions instead of linear functions, that is functions of the form $y=w^T\cdot x+b$ instead of $y=w^T\cdot x$. This can be achieved easily by adding an extra component to the inputs $x^{(i)}$ with a constant value of 1 so that $x\in\mathbb R^{n+1}$ and $w\in\mathbb R^{n+1}$ as well.</p>

<p>In order to compute $\hat w$ analytically, it will be useful to vectorize the setup above. Arrange the $x^{(i)}$ as rows in a matrix $X\in\mathbb R^{m\times (n+1)}$ i.e.</p>

\[X=\begin{bmatrix}
\rule[.5ex]{2.5ex}{0.5pt}{x^{(1)}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\vdots\\
\rule[.5ex]{2.5ex}{0.5pt}{x^{(m)}}^T\rule[.5ex]{2.5ex}{0.5pt}\\
\end{bmatrix}.\]

<p>Similarly, arrange $y^{(i)}$ into a column vector $y\in\mathbb R^{m\times 1}$ i.e.</p>

\[Y=\begin{bmatrix}
y^{(1)}\\
\vdots\\
y^{(m)}
\end{bmatrix}.\]

<p>Then the minimization problem becomes</p>

\[\begin{align*}
\hat w&amp;=\arg\min_w\frac 1m\Vert X\cdot w-y\Vert_2^2\\
&amp;=\arg\min_w\Vert X\cdot w-y\Vert_2^2.
\end{align*}\]

<p>Define</p>

\[f(w):=\Vert X\cdot w-y\Vert_2^2,\quad f:\mathbb R^{n+1}\to\mathbb R.\]

<p>Then computing $\hat w$ becomes a simple calculus problem which can be solved by setting all the partial derivatives of $f$ to 0, and solving for $w$. Recall we have already computed the Jacobian of $f(w)$ (which contains exactly all the partial derivatives of $f$) where we had</p>

\[\begin{align*}
\frac{d}{d w}f(w)&amp;=2(X\cdot w-y)^T\cdot X.
\end{align*}\]

<p>Setting the above equal to 0 and solving for $w$ gives</p>

\[\begin{align*}
2(X\cdot w-y)^T\cdot X&amp;=0\\
\implies(X\cdot w-y)^T\cdot X&amp;=0\\
\implies X^T\cdot (X\cdot w-y)&amp;=0\\
\implies X^T\cdot X\cdot w-X^T\cdot y&amp;=0\\
\implies X^T\cdot X\cdot w&amp;=X^T\cdot y\\
\implies w&amp;=(X^TX)^{-1}X^Ty.
\end{align*}\]

<p>(Technically we should also check that this is indeed a minimum, not a saddle point nor a maximum, by looking at the eigenvalues of the Hessian matrix of $f$)</p>

<h2 id="regularization-ridge-regression">Regularization (Ridge regression)</h2>

<p>A common approach to prevent overfitting in a linear regression model is to modify the function that we are minimizing $w$ over to instead be</p>

\[\hat w:=\arg\min_w\frac 1m\left(\left(\sum_{i=1}^m (w^T\cdot x^{(i)}-y^{(i)})^2\right)+\lambda\Vert w\Vert_2^2\right).\]

<p>where $\lambda\in\mathbb R^+$ is a constant (in practice it’s a hyperparameter that needs to be tuned). The above setup is also called “ridge regression”.</p>

<p>The additional $\lambda\Vert w\Vert_2^2$ term helps to drive down the magnitude of $w$, which causes many components of $w$ to become close to zero. This has a regularizing effect as intuitively, this reduces the model’s complexity, since only a few components of the input are relevant to compute the output.</p>

<p>In matrix form the minimization becomes</p>

\[\begin{align*}
\hat w&amp;=\arg\min_w\frac 1m\left(\Vert X\cdot w-y\Vert_2^2+\lambda \Vert w\Vert_2^2\right)\\
&amp;=\arg\min_w\left(\Vert X\cdot w-y\Vert_2^2+\lambda \Vert w\Vert_2^2\right).
\end{align*}\]

<p>We now approach in the same way as before. Define</p>

\[f(w):=\Vert X\cdot w-y\Vert_2^2+\lambda \Vert w\Vert_2^2.\]

<p>Then</p>

\[\begin{align*}
\frac{d}{d w}\left(\Vert X\cdot w-y\Vert_2^2+\lambda \Vert w\Vert_2^2\right)&amp;=\frac{d(\Vert X\cdot w-y\Vert_2^2))}{dw}+\frac{d(\lambda \Vert w\Vert_2^2)}{dw}\\
&amp;=2(X\cdot w-y)^T\cdot X+\lambda\cdot 2\cdot w^T.
\end{align*}\]

<p>Setting the above equal to 0 and solving for $w$ gives</p>

\[\begin{align*}
2(X\cdot w-y)^T\cdot X+2\lambda\cdot w^T&amp;=0\\
\implies(X\cdot w-y)^T\cdot X+\lambda\cdot w^T&amp;=0\\
\implies  X^T(X\cdot w-y)+\lambda\cdot w&amp;=0\\
\implies X^T\cdot X\cdot w-X^T\cdot y+\lambda\cdot w&amp;=0\\
\implies (X^T\cdot X+\lambda\mathbb I)w&amp;=X^T\cdot y\\
\implies w&amp;=(X^TX+\lambda\mathbb I)^{-1}X^Ty.
\end{align*}\]

      </div>
    </div>
    
    
    <div class="row">
      <div class="col col-12">
        
        <div id="disqus_thread" class="my-5"></div>
        <script>
          /**
          *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
          *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
          var disqus_config = function () {
          this.page.url = "/machine-learning/linear-regression.html";  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = "Linear Regression"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
          };
          (function() { // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          s.src = 'https://pilex-github.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      
      </div>
    </div>
  </div>

  <script>
    $("table").attr("class", "table")
  </script>

  <footer>
    <hr>
    <p class="centered"
      style="margin: 15px; margin-left: auto; margin-right: auto; font-size: 14px; text-align: center">
      Copyright Alex Tan &copy; 2022
    </p>
  </footer>
</body>

</html>