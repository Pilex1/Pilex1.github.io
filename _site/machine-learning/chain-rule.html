<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">

     <link rel="stylesheet" href="/assets/css/styles.css">

     <link rel="icon" type="image/png" href="/assets/img/favicon.png">


  <script src="https://code.jquery.com/jquery-3.3.1.min.js" crossorigin="anonymous"></script>
 
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>


  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.0/math.js"></script>

    
    

</head>

<body>
  <!-- special case to handle the index page -->



    
    

    
    

    
    

    
    
        <h1 class="text-center">Machine Learning</h1>
        <p class="text-center">And human learning too</p>
        
    




<nav class="justify-content-center">
    
        <a href="/index">
            Home
        </a>
    
        <a href="/computer-graphics/index">
            Computer Graphics
        </a>
    
        <a href="/visualizations/index">
            Mathematical Visualizations
        </a>
    
        <a href="/machine-learning/index">
            Machine Learning
        </a>
    
</nav>
  <div class="container">
    <div class="row my-2">
      <div class="col col-lg-6 mx-auto">
        
        
        
    
        
        
    
        
        
    
        
        
            <div class="card">
                <div class="card-body">
                    <h4 class="card-title">Quick links</h4>
                    
                        
                                <a class="d-block" href="/machine-learning/jacobian">Understanding the Jacobian</a>
                        
                    
                        
                            <a class="fw-bold text-light">Chain rule for vector valued functions</a>
                            
                            
                    
                        
                                <a class="d-block" href="/machine-learning/linear-regression">Linear regression</a>
                        
                    
                        
                                <a class="d-block" href="/machine-learning/neural-networks">Neural networks</a>
                        
                    
                </div>
            </div>
        
    
      </div>
    </div>
    <div class="row">
        <div class="col col-12">
            <h1>Chain rule for vector valued functions</h1>
        </div>
    </div>
</div>




  <script id="dsq-count-scr" src="//pilex-github.disqus.com/count.js" async></script>

  <div class="container">
    <div class="row">
      <div class="col col-12">
        <p>We have discussed previously that the notion of a derivative can be generalised to vector functions through the Jacobian matrix. Recall that if $\boldsymbol y=\boldsymbol f(\boldsymbol x)$ where $\boldsymbol f:\mathbb R^n\to\mathbb R^m$ then the Jacobian matrix of $\boldsymbol f$, which we denote here by $\frac{d\boldsymbol f}{d\boldsymbol x}$, is a matrix whose $i$th row and $j$th column is equal to $\frac{\partial f_i}{\partial x_j}$ i.e.</p>

\[\left(\frac{d\boldsymbol f}{d\boldsymbol x}\right)_{i,j}:=\frac{\partial  f_i}{\partial  x_j}.\]

<p>We have already seen previously some examples of Jacobians of common functions. However, when computing Jacobians in practice, the functions we are dealing with are generally quite complex and involve the composition of many simpler functions. For example, as we will see later, in linear regression we need to compute the Jacobian of the function $f(w)=\Vert X\cdot w-y\Vert_2^2,f:\mathbb R^n\to\mathbb R$ where $X,y$ are appropriately sized constants. This is the composition of three functions:</p>

<ul>
  <li>$g_1(w):=X\cdot w$</li>
  <li>$g_2(g_1):=g_1-y$</li>
  <li>$g_3(g_2):=\Vert g_2\Vert_2^2$</li>
</ul>

<p>We know how to compute the Jacobian of each of these three functions individually, but what is the Jacobian when they are composed together?</p>

<p>In single variable calculus, we have the chain rule which describes how to take derivatives of a composition of two or more scalar functions. More precisely, this states that if $y=f(g(x))$ for scalar functions $f,g:\mathbb R\to\mathbb R$ then
\(\frac{dy}{dx}=\frac{df}{dg}\cdot\frac{dg}{dx}\)</p>

<p>where $\cdot$ represents ordinary multiplication.</p>

<p>We will soon see that a very similar statement holds for vector functions as well! Consider the vector case $\boldsymbol y=\boldsymbol f(\boldsymbol g(\boldsymbol x))$ where $\boldsymbol g:\mathbb R^k\to\mathbb R^n$ and $\boldsymbol f:\mathbb R^n\to\mathbb R^m$. Take an arbitrary entry in the Jacobian matrix $\frac{d\boldsymbol y}{d\boldsymbol x}$ at index $i,j$. Then</p>

\[\left(\frac{d\boldsymbol y}{d\boldsymbol x}\right)_{i,j}=\frac{\partial}{\partial x_j}f_i(g_1(x_1,\cdots,x_k),\cdots,g_n(x_1,\cdots,x_k)).\]

<p>By the multivariable chain rule, it follows that</p>

\[\begin{align*}
\left(\frac{d\boldsymbol y}{d\boldsymbol x}\right)_{i,j}&amp;=\frac{\partial f_i}{\partial g_1}\cdot\frac{\partial g_1}{x_j}+\cdots +\frac{\partial f_i}{\partial g_n}\cdot\frac{\partial g_n}{x_j}\\
&amp;=\left(\frac{df}{dg}\right)_{i,1}\cdot\left(\frac{dg}{dx}\right)_{1,j}+\cdots+\left(\frac{df}{dg}\right)_{i,n}\cdot\left(\frac{dg}{dx}\right)_{n,j}
\end{align*}\]

<p>hence</p>

\[\frac{d\boldsymbol y}{d\boldsymbol x}=\frac{d\boldsymbol f}{d\boldsymbol g}\cdot \frac{d\boldsymbol g}{d\boldsymbol x}\]

<p>where $\cdot$ here denotes matrix multiplication.</p>

<p>This is a really neat result as it is the exact analogue of the chain rule in the scalar function case except the derivative is replaced with the Jacobian matrix and the multiplication is replaced by matrix multiplication.</p>

<p>We can use this result to compute the Jacobian of our motivating example of $f(w)=\Vert X\cdot w-y\Vert_2^2$. Since $f(w)=g_3(g_2(g_1(w)))$ we have</p>

\[\begin{align*}
\frac{df}{dw}&amp;=\frac{dg_3}{dg_2}\cdot\frac{dg_2}{dw}\tag{chain rule}\\
&amp;=2g_2^T\cdot\frac{dg_2}{dw}\tag{derivative of squared $L_2$ norm}\\
&amp;=2g_2^T\cdot \frac{dg_2}{dg_1}\cdot\frac{dg_1}{dw}\tag{chain rule}\\
&amp;=2g_2^T\cdot\mathbb I\cdot\frac{dg_1}{dw}\tag{derivative of addition of a constant}\\
&amp;=2g_2^T\cdot\mathbb I\cdot X\tag{derivative of left-multiplication by a matrix}\\
&amp;=2(X\cdot w-y)^T\cdot X.
\end{align*}\]

      </div>
    </div>
    
    
    <div class="row">
      <div class="col col-12">
        
        <div id="disqus_thread" class="my-5"></div>
        <script>
          /**
          *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
          *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
          var disqus_config = function () {
          this.page.url = "/machine-learning/chain-rule.html";  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = "Chain Rule"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
          };
          (function() { // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          s.src = 'https://pilex-github.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      
      </div>
    </div>
  </div>

  <script>
    $("table").attr("class", "table")
  </script>

  <footer>
    <hr>
    <p class="centered"
      style="margin: 15px; margin-left: auto; margin-right: auto; font-size: 14px; text-align: center">
      Copyright Alex Tan &copy; 2022
    </p>
  </footer>
</body>

</html>