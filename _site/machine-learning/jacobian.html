<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">

  <link rel="stylesheet" href="/assets/css/styles.css">

  <link rel="icon" type="image/png" href="/assets/img/favicon.png">


  <script src="https://code.jquery.com/jquery-3.3.1.min.js" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
    crossorigin="anonymous"></script>


  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      startup: {
        ready: () => {
          MathJax.startup.defaultReady();
          MathJax.startup.promise.then(() => {
            // console.log('MathJax initial typesetting complete');

            // a hack, but you need this for lines created using \rule to be rendered in the correct colour
            $("rect").removeAttr("fill");
          });
        }
      }

    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.0/math.js"></script>

  
  

</head>

<body>
  <!-- special case to handle the index page -->



    
    

    
    

    
    

    
    
        <h1 class="text-center">Machine Learning</h1>
        <p class="text-center">And human learning too</p>
        
    




<nav class="justify-content-center">
    
        <a href="/index">
            Home
        </a>
    
        <a href="/computer-graphics/index">
            Computer Graphics
        </a>
    
        <a href="/visualizations/index">
            Mathematical Visualizations
        </a>
    
        <a href="/machine-learning/index">
            Machine Learning
        </a>
    
</nav>
  <div class="container">
    <div class="row my-2">
        <div class="col col-lg-6 mx-auto">
            
            
            
            
            
            
            
            
            
            
            
            
            <div class="card">
                <div class="card-body">
                    <h4 class="card-title">Quick links</h4>
                    
                    
                    
                    <a class="fw-bold text-light">Jacobian matrix</a>

                    

                    
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/chain-rule">Chain rule for vector-to-vector functions</a>
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/linear-regression">Linear regression</a>
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/neural-networks-1">Neural networks part 1</a>
                    
                    
                    
                    
                    <a class="d-block" href="/machine-learning/neural-networks-2">Neural networks part 2</a>
                    
                    
                </div>
            </div>
            
            
        </div>
    </div>
    <div class="row">
        <div class="col col-12">
            <h1>Jacobian matrix</h1>
        </div>
    </div>
</div>

  <script id="dsq-count-scr" src="//pilex-github.disqus.com/count.js" async></script>

  <div class="container">
    <div class="row">
      <div class="col col-12">
        <p>Suppose $\boldsymbol f$ is a function that has vectors for both input and output i.e.  $\boldsymbol f:\mathbb R^n\to\mathbb R^m$. We generalise the notion of a derivative of $\boldsymbol f$ by defining the Jacobian matrix (also called “derivative matrix”) of $\boldsymbol f$ which we denote here by $\frac{d\boldsymbol f}{d\boldsymbol x}$. This is a matrix consisting of all the partial derivatives of all the component functions of $\boldsymbol f$. More precisely, if we let $(\frac{d\boldsymbol f}{d\boldsymbol x})_{i,j}$ denote the $i$th row and $j$th column of $\frac{d\boldsymbol f}{d\boldsymbol x}$ then</p>

\[\left(\frac{d\boldsymbol f}{d\boldsymbol x}\right)_{i,j}:=\frac{\partial  f_i}{\partial  x_j}.\]

<p>Explicitly,</p>

\[\frac{d\boldsymbol f}{d\boldsymbol x}=\begin{bmatrix}
\frac{\partial f_1}{\partial x_1}&amp;\cdots&amp;\frac{\partial f_1}{\partial x_n}\\
\vdots&amp;\ddots&amp;\vdots\\
\frac{\partial f_m}{\partial x_1}&amp;\cdots&amp;\frac{\partial f_m}{\partial x_n}.
\end{bmatrix}\]

<p>Intuitively, each column $j$ tells you the factor by which space in each of the output coordinates changes per infinitesimal change in $x_j$. The determinant of the Jacobian matrix then tells you the factor by which area/volume of the output space $\boldsymbol f$ changes per change in infinitesimal area/volume in the input space.</p>

<p>For example, consider the function</p>

\[\boldsymbol f\left(x,y\right)=\begin{bmatrix}f_1(x,y)\\f_2(x,y)\end{bmatrix}=\begin{bmatrix}x+y\\xy\end{bmatrix}\]

<p>The Jacobian is</p>

\[\begin{bmatrix}
\frac{\partial f_1}{\partial x}&amp;\frac{\partial f_1}{\partial y}\\
\frac{\partial f_2}{\partial x}&amp;\frac{\partial f_2}{\partial y}\\
\end{bmatrix}=\begin{bmatrix}
1&amp;1\\
y&amp;x\\
\end{bmatrix}\]

<p>Let’s pick a point in the input space arbitrarily say $(x,y)=(-1,2)$. Then the Jacobian evaluated at this point is</p>

\[\begin{bmatrix}1&amp;1\\2&amp;-1\end{bmatrix}\]

<p>The first column of this matrix tells us that for an infinitesimal change in $x$ around $(x,y)=(-1,2)$, in the output space $f_1$ changes by a factor of 1 (i.e. it changes by the same amount) and $f_2$ changes by a factor of 2 (it increases twice as much), Similarly, the second column of the matrix tells us that for an infinitesimal change in $y$ around $(x,y)=(-1,2)$, in the output space $f_1$ changes by a factor of 1 and $f_2$ changes by a factor of -1.</p>

<p>Hence the Jacobian matrix as a whole tells us that an infinitesimal change in the input space at $(x,y)=(-1,2)$ has the effect of producing a linear change in the output space described by the matrix</p>

\[\begin{bmatrix}1&amp;1\\2&amp;-1\end{bmatrix}.\]

<p>To illustrate this, note that $f(-1,2)=(1,-2)$. Suppose we perturb the input slightly to $(x,y)=(-1+0.001,2-0.0005)=(-0.999,1.9995)$. Then the Jacobian matrix tells us that the change in the output space is approximately</p>

\[\begin{bmatrix}
1&amp;1\\
2&amp;-1
\end{bmatrix}\cdot\begin{bmatrix}
0.001\\
-0.0005
\end{bmatrix}=\begin{bmatrix}
0.0005\\
0.0025
\end{bmatrix}\]

<p>meaning that we should expect $f(-0.999,1.9995)\approx (1,-2)+(0.0005,0.0025)=(1.0005,-1.9975)$. And indeed the true value is $f(-0.999,1.9995)=(1.0005,-1.9975005)$.</p>

<p>Note also that the determinant of the Jacobian matrix is $-3$. This means if you were to consider the perturbation as an area (i.e. consider the square with corners given by $(x,y)=(-1,2)$ and $(x,y)=(-0.999,1.9995$) and apply $\boldsymbol f$ to this area, the resulting shape would be approximately $3$ times as large as the input, and the orientation would be flipped (due to the negative sign).</p>

<h2 id="examples">Examples</h2>

<p>Here we will work through computing the Jacobian matrices of functions that pop up frequently in neural network maths.</p>

<h3 id="left-multiplication-by-matrix">Left-multiplication by matrix</h3>

<p>Let</p>

\[f(x):=W\cdot x,\quad f:\mathbb R^n\to\mathbb R^m\]

<p>where $W\in\mathbb R^{m\times n}$. Then</p>

\[\begin{align*}
\frac{\partial f_i}{\partial x_j}&amp;=\frac{\partial}{\partial x_j}\left(W_{i,1}x_1+\cdots+W_{i,n}x_n\right)\\
&amp;=\frac{\partial}{\partial x_j}W_{i,j}x_j\\
&amp;=W_{i,j}
\end{align*}\]

<p>hence</p>

\[\frac{df}{dx}=W.\]

<h3 id="squared-l_2-norm">Squared $L_2$ norm</h3>

<p>Let</p>

\[f(x)=\Vert x\Vert_2^2,\quad f:\mathbb R^n\to\mathbb R.\]

<p>Then</p>

\[\begin{align*}
\frac{\partial f}{\partial x_j}&amp;=\frac{\partial}{\partial x_j}\left(x_1^2+\cdots+x_n^2\right)\\
&amp;=\frac{\partial}{\partial x_j}x_j^2\\
&amp;=2x_j
\end{align*}\]

<p>hence</p>

\[\frac{df}{dx}=2x^T.\]

<h3 id="elementwise-functions">Elementwise functions</h3>

<p>Let $g\circ x$ denote the function $g:\mathbb R\to\mathbb R$ being applied elementwise to $x$. Let</p>

\[f(x)=g\circ x,\quad f:\mathbb R^n\to\mathbb R^n.\]

<p>Then</p>

\[\begin{align*}
\frac{\partial f_i}{\partial x_j}&amp;=\frac{\partial}{\partial x_j}\left(g(x_i)\right)\\
&amp;=g'(x_i)1_{i=j}
\end{align*}\]

<p>hence $\frac{df}{dx}$ is a diagonal matrix with its diagonal entries given by the vector $g’\circ x$. For ease of notation if $u$ is a vector, we define $\text{diag}(u)$ to be the diagonal matrix whose diagonal entries are given by the components of $u$. With this notation, we have that</p>

\[\frac{df}{dx}=\text{diag}(g'\circ x).\]

<p>Using this result we can easily deduce the following two results.</p>

<h4 id="elementwise-multiplication-by-a-constant">Elementwise multiplication by a constant</h4>

<p>Let</p>

\[f(x)=x\odot c,\quad f:\mathbb R^n\to\mathbb R^n\]

<p>where $c\in\mathbb R^n$ and $x\odot c$ denotes elementwise multiplication between $x$ and $c$. Then with the previous notation, the elementwise function is $g(x)=x\cdot c$ and so $g’(x)=c$. Hence</p>

\[\frac{df}{dx}=\text{diag}(c).\]

<h4 id="elementwise-addition-by-a-constant">Elementwise addition by a constant</h4>

<p>Let</p>

\[f(x)=x+c,\quad f:\mathbb R^n\to\mathbb R^n\]

<p>where $c\in\mathbb R^n$. Then with the previous notation, the elementwise function is $g(x)=x+c$ and so $g’(x)=1$. Hence</p>

\[\frac{df}{dx}=\mathbb I_n\]

<p>the $n\times n$ identity matrix.</p>

      </div>
    </div>
    
    
    <div class="row">
      <div class="col col-12">
        
        <div id="disqus_thread" class="my-5"></div>
        <script>
          /**
          *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
          *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
          var disqus_config = function () {
            this.page.url = "/machine-learning/jacobian.html";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = "Jacobian"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
          };
          (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://pilex-github.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        

      </div>
    </div>
  </div>

  <script>
    $("table").attr("class", "table");
  </script>

  <footer>
    <hr>
    <p class="centered"
      style="margin: 15px; margin-left: auto; margin-right: auto; font-size: 14px; text-align: center">
      Copyright Alex Tan &copy; 2022
    </p>
  </footer>
</body>

</html>